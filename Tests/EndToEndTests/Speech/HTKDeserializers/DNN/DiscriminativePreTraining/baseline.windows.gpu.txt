CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU W3530 @ 2.80GHz
    Hardware threads: 4
    Total Memory: 12580404 kB
-------------------------------------------------------------------
=== Running /cygdrive/c/jenkins/workspace/CNTK-Test-Windows-W1/x64/release/cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/cntk_dpt.cntk currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 07:04:27
		Last modified date: Wed Jul 13 07:40:26 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: c:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: b918e06de12613a1773a0976ab94e213bd09ce52
		Built by svcphil on cntk-muc01
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
07/14/2016 07:51:25: -------------------------------------------------------------------
07/14/2016 07:51:25: Build info: 

07/14/2016 07:51:25: 		Built time: Jul 14 2016 07:04:27
07/14/2016 07:51:25: 		Last modified date: Wed Jul 13 07:40:26 2016
07/14/2016 07:51:25: 		Build type: Release
07/14/2016 07:51:25: 		Build target: GPU
07/14/2016 07:51:25: 		With 1bit-SGD: no
07/14/2016 07:51:25: 		Math lib: mkl
07/14/2016 07:51:25: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
07/14/2016 07:51:25: 		CUB_PATH: c:\src\cub-1.4.1
07/14/2016 07:51:25: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
07/14/2016 07:51:25: 		Build Branch: HEAD
07/14/2016 07:51:25: 		Build SHA1: b918e06de12613a1773a0976ab94e213bd09ce52
07/14/2016 07:51:25: 		Built by svcphil on cntk-muc01
07/14/2016 07:51:25: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
07/14/2016 07:51:25: -------------------------------------------------------------------
07/14/2016 07:51:25: -------------------------------------------------------------------
07/14/2016 07:51:25: GPU info:

07/14/2016 07:51:25: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8090 MB
07/14/2016 07:51:25: -------------------------------------------------------------------

07/14/2016 07:51:25: Running on cntk-muc00 at 2016/07/14 07:51:25
07/14/2016 07:51:25: Command line: 
C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/cntk_dpt.cntk  currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu  DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu  DeviceId=0  timestamping=true



07/14/2016 07:51:25: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
07/14/2016 07:51:25: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu
DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true

07/14/2016 07:51:25: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

07/14/2016 07:51:25: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
07/14/2016 07:51:25: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf"
        labelMappingFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu
DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true

07/14/2016 07:51:25: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

07/14/2016 07:51:25: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
configparameters: cntk_dpt.cntk:currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
configparameters: cntk_dpt.cntk:DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
configparameters: cntk_dpt.cntk:deviceId=0
configparameters: cntk_dpt.cntk:dptPre1=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:dptPre2=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.cntk:ndlMacros=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:precision=float
configparameters: cntk_dpt.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf"
        labelMappingFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_dpt.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.cntk:speechTrain=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.cntk:timestamping=true
configparameters: cntk_dpt.cntk:traceLevel=1
07/14/2016 07:51:25: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
07/14/2016 07:51:25: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain
07/14/2016 07:51:25: Precision = "float"
07/14/2016 07:51:25: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech
07/14/2016 07:51:25: CNTKCommandTrainInfo: dptPre1 : 2
07/14/2016 07:51:25: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech
07/14/2016 07:51:25: CNTKCommandTrainInfo: dptPre2 : 2
07/14/2016 07:51:25: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech
07/14/2016 07:51:25: CNTKCommandTrainInfo: speechTrain : 4
07/14/2016 07:51:25: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8

07/14/2016 07:51:25: ##############################################################################
07/14/2016 07:51:25: #                                                                            #
07/14/2016 07:51:25: # Action "train"                                                             #
07/14/2016 07:51:25: #                                                                            #
07/14/2016 07:51:25: ##############################################################################

07/14/2016 07:51:25: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 946 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252508 frames in 946 out of 946 utterances
label set 0: 129 classes
minibatchutterancesource: 946 utterances grouped into 3 chunks, av. chunk size: 315.3 utterances, 84169.3 frames

07/14/2016 07:51:26: Creating virgin network.
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/14/2016 07:51:26: Created model with 19 nodes on GPU 0.

07/14/2016 07:51:26: Training criterion node(s):
07/14/2016 07:51:26: 	ce = CrossEntropyWithSoftmax

07/14/2016 07:51:26: Evaluation criterion node(s):

07/14/2016 07:51:26: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *]] [features Gradient[363 x *]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *]] }
00000007D0C10380: {[HL1.b Value[512 x 1]] }
00000007D0C106A0: {[globalMean Value[363 x 1]] }
00000007D0C10740: {[HL1.W Value[512 x 363]] }
00000007D0C10920: {[globalInvStd Value[363 x 1]] }
00000007D0C110A0: {[labels Value[132 x *]] }
00000007D0C11500: {[OL.W Value[132 x 512]] }
00000007D0C116E0: {[globalPrior Value[132 x 1]] }
00000007D0C11A00: {[OL.b Value[132 x 1]] }
00000007D37C55A0: {[ce Gradient[1]] }
00000007D37C5780: {[ce Value[1]] }
00000007D37C5A00: {[err Value[1]] }
00000007D37C5AA0: {[logPrior Value[132 x 1]] }
00000007D37C5E60: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *]] }
00000007D37C6040: {[OL.t Gradient[132 x 1 x *]] }
00000007D37C6180: {[HL1.t Value[512 x *]] }
00000007D37C6220: {[OL.b Gradient[132 x 1]] }
00000007D37C6400: {[HL1.z Gradient[512 x 1 x *]] [OL.t Value[132 x 1 x *]] }
00000007D37C6860: {[HL1.t Gradient[512 x *]] [HL1.y Value[512 x 1 x *]] }
00000007D37C6A40: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *]] [OL.z Gradient[132 x 1 x *]] }
00000007D37C6AE0: {[scaledLogLikelihood Value[132 x 1 x *]] }
00000007D37C6CC0: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *]] }
00000007D37C6FE0: {[featNorm Value[363 x *]] }
00000007D66BFEC0: {[features Value[363 x *]] }

07/14/2016 07:51:26: No PreCompute nodes found, skipping PreCompute step.

07/14/2016 07:51:26: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/14/2016 07:51:27: Starting minibatch loop.
07/14/2016 07:51:27:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.79400444 * 2560; err = 0.82460937 * 2560; time = 0.4250s; samplesPerSecond = 6023.5
07/14/2016 07:51:27:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.98001518 * 2560; err = 0.69882813 * 2560; time = 0.0140s; samplesPerSecond = 183171.2
07/14/2016 07:51:27:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.43603897 * 2560; err = 0.62539062 * 2560; time = 0.0140s; samplesPerSecond = 183197.4
07/14/2016 07:51:27:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.20699692 * 2560; err = 0.58242187 * 2560; time = 0.0139s; samplesPerSecond = 184464.6
07/14/2016 07:51:27:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.98738327 * 2560; err = 0.53984375 * 2560; time = 0.0140s; samplesPerSecond = 183118.7
07/14/2016 07:51:27:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.90868073 * 2560; err = 0.52187500 * 2560; time = 0.0140s; samplesPerSecond = 182844.1
07/14/2016 07:51:27:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.81027527 * 2560; err = 0.50390625 * 2560; time = 0.0140s; samplesPerSecond = 182297.2
07/14/2016 07:51:27:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.71633606 * 2560; err = 0.47617188 * 2560; time = 0.0140s; samplesPerSecond = 183302.3
07/14/2016 07:51:27:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.63621521 * 2560; err = 0.47890625 * 2560; time = 0.0140s; samplesPerSecond = 183079.5
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.63022461 * 2560; err = 0.46875000 * 2560; time = 0.0140s; samplesPerSecond = 182700.5
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.54549561 * 2560; err = 0.44453125 * 2560; time = 0.0140s; samplesPerSecond = 183341.7
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.44241486 * 2560; err = 0.42382813 * 2560; time = 0.0142s; samplesPerSecond = 180893.2
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.48433990 * 2560; err = 0.42968750 * 2560; time = 0.0140s; samplesPerSecond = 182557.2
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.47519531 * 2560; err = 0.43671875 * 2560; time = 0.0140s; samplesPerSecond = 183171.2
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.42121277 * 2560; err = 0.41445312 * 2560; time = 0.0140s; samplesPerSecond = 182739.7
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.44315796 * 2560; err = 0.42773438 * 2560; time = 0.0139s; samplesPerSecond = 183776.0
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.45713806 * 2560; err = 0.43398437 * 2560; time = 0.0139s; samplesPerSecond = 183960.9
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.41806641 * 2560; err = 0.41601563 * 2560; time = 0.0139s; samplesPerSecond = 184650.9
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.38606262 * 2560; err = 0.41015625 * 2560; time = 0.0139s; samplesPerSecond = 183512.5
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.35830688 * 2560; err = 0.39687500 * 2560; time = 0.0140s; samplesPerSecond = 183014.0
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.35883789 * 2560; err = 0.41015625 * 2560; time = 0.0140s; samplesPerSecond = 183158.0
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.35698242 * 2560; err = 0.40625000 * 2560; time = 0.0139s; samplesPerSecond = 184199.2
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.25728455 * 2560; err = 0.39179687 * 2560; time = 0.0139s; samplesPerSecond = 183974.1
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.28092651 * 2560; err = 0.38242188 * 2560; time = 0.0140s; samplesPerSecond = 183210.5
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.37520142 * 2560; err = 0.43007812 * 2560; time = 0.0140s; samplesPerSecond = 182778.8
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.38888550 * 2560; err = 0.40078125 * 2560; time = 0.0140s; samplesPerSecond = 183262.9
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.33003845 * 2560; err = 0.40546875 * 2560; time = 0.0139s; samplesPerSecond = 183631.0
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.28875427 * 2560; err = 0.39726563 * 2560; time = 0.0140s; samplesPerSecond = 182987.8
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.23384094 * 2560; err = 0.37148437 * 2560; time = 0.0140s; samplesPerSecond = 183499.4
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.24738464 * 2560; err = 0.37539062 * 2560; time = 0.0139s; samplesPerSecond = 183512.5
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.24707031 * 2560; err = 0.37734375 * 2560; time = 0.0140s; samplesPerSecond = 182687.5
07/14/2016 07:51:28:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.29297791 * 2560; err = 0.40195313 * 2560; time = 0.0141s; samplesPerSecond = 181418.8
07/14/2016 07:51:28: Finished Epoch[ 1 of 2]: [Training] ce = 1.63111706 * 81920; err = 0.45953369 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.45781s
07/14/2016 07:51:28: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech.1'

07/14/2016 07:51:28: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/14/2016 07:51:28: Starting minibatch loop.
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.24797297 * 2560; err = 0.39609375 * 2560; time = 0.0150s; samplesPerSecond = 170803.3
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.19887552 * 2560; err = 0.36757812 * 2560; time = 0.0141s; samplesPerSecond = 181059.5
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.22719784 * 2560; err = 0.37382813 * 2560; time = 0.0140s; samplesPerSecond = 182427.1
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.21818581 * 2560; err = 0.36484375 * 2560; time = 0.0140s; samplesPerSecond = 182531.2
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.27766418 * 2560; err = 0.38710937 * 2560; time = 0.0140s; samplesPerSecond = 182453.1
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.27238350 * 2560; err = 0.37460938 * 2560; time = 0.0139s; samplesPerSecond = 183960.9
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.21777573 * 2560; err = 0.36953125 * 2560; time = 0.0140s; samplesPerSecond = 183446.8
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.19838867 * 2560; err = 0.36835937 * 2560; time = 0.0140s; samplesPerSecond = 183446.8
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.24918365 * 2560; err = 0.38828125 * 2560; time = 0.0139s; samplesPerSecond = 183525.7
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.22839661 * 2560; err = 0.36367187 * 2560; time = 0.0139s; samplesPerSecond = 184331.8
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.16281128 * 2560; err = 0.35000000 * 2560; time = 0.0139s; samplesPerSecond = 183868.4
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.20102234 * 2560; err = 0.36601563 * 2560; time = 0.0139s; samplesPerSecond = 183670.5
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.28279114 * 2560; err = 0.39804688 * 2560; time = 0.0139s; samplesPerSecond = 184106.4
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.26652374 * 2560; err = 0.39570312 * 2560; time = 0.0139s; samplesPerSecond = 184080.0
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.23302460 * 2560; err = 0.38046875 * 2560; time = 0.0139s; samplesPerSecond = 183525.7
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.20618134 * 2560; err = 0.36367187 * 2560; time = 0.0140s; samplesPerSecond = 182492.2
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.13449097 * 2560; err = 0.34843750 * 2560; time = 0.0139s; samplesPerSecond = 184464.6
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.20425262 * 2560; err = 0.36289063 * 2560; time = 0.0140s; samplesPerSecond = 182336.2
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.13072968 * 2560; err = 0.34882812 * 2560; time = 0.0140s; samplesPerSecond = 182857.1
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.15933685 * 2560; err = 0.34648438 * 2560; time = 0.0140s; samplesPerSecond = 182752.7
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.17813110 * 2560; err = 0.36718750 * 2560; time = 0.0140s; samplesPerSecond = 183105.6
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.15172272 * 2560; err = 0.35195312 * 2560; time = 0.0139s; samplesPerSecond = 183670.5
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.09606018 * 2560; err = 0.33359375 * 2560; time = 0.0139s; samplesPerSecond = 184810.9
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.13315430 * 2560; err = 0.34140625 * 2560; time = 0.0139s; samplesPerSecond = 184146.2
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.13280029 * 2560; err = 0.35664062 * 2560; time = 0.0140s; samplesPerSecond = 183394.2
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.09339600 * 2560; err = 0.32187500 * 2560; time = 0.0139s; samplesPerSecond = 183947.7
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.05860596 * 2560; err = 0.33632812 * 2560; time = 0.0141s; samplesPerSecond = 180944.3
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.09017029 * 2560; err = 0.33593750 * 2560; time = 0.0142s; samplesPerSecond = 180905.9
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.21965332 * 2560; err = 0.36289063 * 2560; time = 0.0141s; samplesPerSecond = 181393.0
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.11653442 * 2560; err = 0.34492187 * 2560; time = 0.0141s; samplesPerSecond = 181072.3
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.17740479 * 2560; err = 0.35781250 * 2560; time = 0.0142s; samplesPerSecond = 180829.3
07/14/2016 07:51:28:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.11124573 * 2560; err = 0.34257813 * 2560; time = 0.0142s; samplesPerSecond = 180472.3
07/14/2016 07:51:28: Finished Epoch[ 2 of 2]: [Training] ce = 1.18362713 * 81920; err = 0.36148682 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.451872s
07/14/2016 07:51:28: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech'
07/14/2016 07:51:28: CNTKCommandTrainEnd: dptPre1

07/14/2016 07:51:28: Action "train" complete.


07/14/2016 07:51:28: ##############################################################################
07/14/2016 07:51:28: #                                                                            #
07/14/2016 07:51:28: # Action "edit"                                                              #
07/14/2016 07:51:28: #                                                                            #
07/14/2016 07:51:28: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/14/2016 07:51:28: Action "edit" complete.


07/14/2016 07:51:28: ##############################################################################
07/14/2016 07:51:28: #                                                                            #
07/14/2016 07:51:28: # Action "train"                                                             #
07/14/2016 07:51:28: #                                                                            #
07/14/2016 07:51:28: ##############################################################################

07/14/2016 07:51:28: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 946 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252508 frames in 946 out of 946 utterances
label set 0: 129 classes
minibatchutterancesource: 946 utterances grouped into 3 chunks, av. chunk size: 315.3 utterances, 84169.3 frames

07/14/2016 07:51:29: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/14/2016 07:51:29: Loaded model with 24 nodes on GPU 0.

07/14/2016 07:51:29: Training criterion node(s):
07/14/2016 07:51:29: 	ce = CrossEntropyWithSoftmax

07/14/2016 07:51:29: Evaluation criterion node(s):

07/14/2016 07:51:29: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *3]] [features Gradient[363 x *3]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *3]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *3]] }
00000007CA690BA0: {[ce Gradient[1]] }
00000007CA690CE0: {[OL.t Gradient[132 x 1 x *3]] }
00000007CA691000: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *3]] [OL.z Gradient[132 x 1 x *3]] }
00000007D0C10EC0: {[labels Value[132 x *3]] }
00000007D0C10F60: {[err Value[1]] }
00000007D0C11640: {[OL.b Value[132 x 1]] }
00000007D0C11780: {[OL.W Value[132 x 512]] }
00000007D0C11A00: {[scaledLogLikelihood Value[132 x 1 x *3]] }
00000007D0C11AA0: {[ce Value[1]] }
00000007D0C11E60: {[logPrior Value[132 x 1]] }
00000007D37C5780: {[globalMean Value[363 x 1]] }
00000007D37C5AA0: {[HL1.b Value[512 x 1]] }
00000007D37C6040: {[HL2.b Value[512 x 1]] }
00000007D37C6400: {[features Value[363 x *3]] }
00000007D37C6AE0: {[HL1.W Value[512 x 363]] }
00000007D37C6C20: {[HL2.W Value[512 x 512]] }
00000007D37C7120: {[globalInvStd Value[363 x 1]] }
00000007D37C71C0: {[globalPrior Value[132 x 1]] }
00000007D66BFEC0: {[OL.b Gradient[132 x 1]] }
00000007E527D2C0: {[HL2.t Gradient[512 x 1 x *3]] [HL2.y Value[512 x 1 x *3]] }
00000007E527D400: {[HL1.z Gradient[512 x 1 x *3]] [HL2.t Value[512 x 1 x *3]] }
00000007E527DB80: {[HL1.t Value[512 x *3]] }
00000007E527E080: {[HL1.t Gradient[512 x *3]] [HL1.y Value[512 x 1 x *3]] }
00000007E527E120: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *3]] [HL2.z Gradient[512 x 1 x *3]] [OL.t Value[132 x 1 x *3]] }
00000007E527E1C0: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *3]] }
00000007E527E940: {[featNorm Value[363 x *3]] }
00000007E527EA80: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *3]] }
00000007E527ED00: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *3]] }

07/14/2016 07:51:29: No PreCompute nodes found, skipping PreCompute step.

07/14/2016 07:51:29: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/14/2016 07:51:29: Starting minibatch loop.
07/14/2016 07:51:29:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.70093880 * 2560; err = 0.79257813 * 2560; time = 0.0235s; samplesPerSecond = 108751.1
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.55858955 * 2560; err = 0.63867188 * 2560; time = 0.0214s; samplesPerSecond = 119497.7
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.01844559 * 2560; err = 0.54843750 * 2560; time = 0.0215s; samplesPerSecond = 119158.4
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.82977219 * 2560; err = 0.51367188 * 2560; time = 0.0214s; samplesPerSecond = 119637.3
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.66964874 * 2560; err = 0.47031250 * 2560; time = 0.0215s; samplesPerSecond = 119325.1
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.62096329 * 2560; err = 0.48242188 * 2560; time = 0.0215s; samplesPerSecond = 119053.2
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.51650543 * 2560; err = 0.43085937 * 2560; time = 0.0215s; samplesPerSecond = 119225.0
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.47762299 * 2560; err = 0.42109375 * 2560; time = 0.0214s; samplesPerSecond = 119531.2
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.40691681 * 2560; err = 0.42304687 * 2560; time = 0.0215s; samplesPerSecond = 119280.6
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.41916199 * 2560; err = 0.41640625 * 2560; time = 0.0215s; samplesPerSecond = 119308.4
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.33325043 * 2560; err = 0.39101562 * 2560; time = 0.0215s; samplesPerSecond = 118926.0
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.28030701 * 2560; err = 0.37148437 * 2560; time = 0.0214s; samplesPerSecond = 119352.9
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.33150024 * 2560; err = 0.40664062 * 2560; time = 0.0214s; samplesPerSecond = 119492.2
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.31927338 * 2560; err = 0.40039063 * 2560; time = 0.0214s; samplesPerSecond = 119626.2
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.27035675 * 2560; err = 0.36835937 * 2560; time = 0.0215s; samplesPerSecond = 119213.9
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.29555664 * 2560; err = 0.38554688 * 2560; time = 0.0214s; samplesPerSecond = 119754.9
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.29791565 * 2560; err = 0.39218750 * 2560; time = 0.0214s; samplesPerSecond = 119458.7
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.25495911 * 2560; err = 0.37734375 * 2560; time = 0.0214s; samplesPerSecond = 119548.0
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.25442810 * 2560; err = 0.37812500 * 2560; time = 0.0215s; samplesPerSecond = 119197.3
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.23615723 * 2560; err = 0.36914063 * 2560; time = 0.0214s; samplesPerSecond = 119581.5
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.23572998 * 2560; err = 0.37265625 * 2560; time = 0.0215s; samplesPerSecond = 119347.3
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.26311340 * 2560; err = 0.37070313 * 2560; time = 0.0215s; samplesPerSecond = 119152.9
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.16498718 * 2560; err = 0.35390625 * 2560; time = 0.0214s; samplesPerSecond = 119548.0
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.17894897 * 2560; err = 0.35234375 * 2560; time = 0.0215s; samplesPerSecond = 119180.6
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.26034241 * 2560; err = 0.38945313 * 2560; time = 0.0215s; samplesPerSecond = 119053.2
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.27960510 * 2560; err = 0.38359375 * 2560; time = 0.0214s; samplesPerSecond = 119408.6
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.22523193 * 2560; err = 0.37304688 * 2560; time = 0.0214s; samplesPerSecond = 119464.3
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.21058960 * 2560; err = 0.37031250 * 2560; time = 0.0214s; samplesPerSecond = 119497.7
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.18031311 * 2560; err = 0.35468750 * 2560; time = 0.0214s; samplesPerSecond = 119347.3
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.16094360 * 2560; err = 0.35585937 * 2560; time = 0.0215s; samplesPerSecond = 119091.9
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.17715149 * 2560; err = 0.35429688 * 2560; time = 0.0214s; samplesPerSecond = 119364.0
07/14/2016 07:51:30:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.22115784 * 2560; err = 0.37382813 * 2560; time = 0.0214s; samplesPerSecond = 119492.2
07/14/2016 07:51:30: Finished Epoch[ 1 of 2]: [Training] ce = 1.45782452 * 81920; err = 0.41820068 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.858239s
07/14/2016 07:51:30: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.1'

07/14/2016 07:51:30: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/14/2016 07:51:30: Starting minibatch loop.
07/14/2016 07:51:30:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.19346094 * 2560; err = 0.36640625 * 2560; time = 0.0221s; samplesPerSecond = 115758.5
07/14/2016 07:51:30:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.15936079 * 2560; err = 0.35507813 * 2560; time = 0.0215s; samplesPerSecond = 119186.2
07/14/2016 07:51:30:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.11364822 * 2560; err = 0.33554688 * 2560; time = 0.0214s; samplesPerSecond = 119626.2
07/14/2016 07:51:30:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.12370262 * 2560; err = 0.33554688 * 2560; time = 0.0215s; samplesPerSecond = 119275.0
07/14/2016 07:51:30:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.16927223 * 2560; err = 0.35859375 * 2560; time = 0.0215s; samplesPerSecond = 119152.9
07/14/2016 07:51:30:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.17904396 * 2560; err = 0.35468750 * 2560; time = 0.0214s; samplesPerSecond = 119659.7
07/14/2016 07:51:30:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.14478226 * 2560; err = 0.34687500 * 2560; time = 0.0215s; samplesPerSecond = 119247.3
07/14/2016 07:51:30:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.12538834 * 2560; err = 0.34531250 * 2560; time = 0.0215s; samplesPerSecond = 119263.9
07/14/2016 07:51:30:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.18400497 * 2560; err = 0.36289063 * 2560; time = 0.0214s; samplesPerSecond = 119553.5
07/14/2016 07:51:30:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.15700989 * 2560; err = 0.34843750 * 2560; time = 0.0215s; samplesPerSecond = 118970.2
07/14/2016 07:51:30:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.09975510 * 2560; err = 0.33007813 * 2560; time = 0.0214s; samplesPerSecond = 119481.0
07/14/2016 07:51:30:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.15562439 * 2560; err = 0.35546875 * 2560; time = 0.0215s; samplesPerSecond = 119325.1
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.14950256 * 2560; err = 0.34843750 * 2560; time = 0.0215s; samplesPerSecond = 119269.5
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.13352966 * 2560; err = 0.35546875 * 2560; time = 0.0215s; samplesPerSecond = 119186.2
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.12802887 * 2560; err = 0.35507813 * 2560; time = 0.0215s; samplesPerSecond = 119008.9
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.12087860 * 2560; err = 0.33867188 * 2560; time = 0.0214s; samplesPerSecond = 119364.0
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.08560333 * 2560; err = 0.32695313 * 2560; time = 0.0215s; samplesPerSecond = 119330.6
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.17805634 * 2560; err = 0.35507813 * 2560; time = 0.0214s; samplesPerSecond = 119397.4
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.07586060 * 2560; err = 0.33085938 * 2560; time = 0.0214s; samplesPerSecond = 119458.7
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.11368408 * 2560; err = 0.33554688 * 2560; time = 0.0215s; samplesPerSecond = 119114.1
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.12821198 * 2560; err = 0.35078125 * 2560; time = 0.0214s; samplesPerSecond = 119525.6
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.11865845 * 2560; err = 0.34101562 * 2560; time = 0.0215s; samplesPerSecond = 119230.6
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.07087860 * 2560; err = 0.33320312 * 2560; time = 0.0214s; samplesPerSecond = 119514.5
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.11179199 * 2560; err = 0.35000000 * 2560; time = 0.0215s; samplesPerSecond = 119230.6
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.09218750 * 2560; err = 0.33164063 * 2560; time = 0.0215s; samplesPerSecond = 119097.5
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.05120544 * 2560; err = 0.30507812 * 2560; time = 0.0214s; samplesPerSecond = 119453.1
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.03429871 * 2560; err = 0.33164063 * 2560; time = 0.0215s; samplesPerSecond = 119341.8
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.06539917 * 2560; err = 0.32812500 * 2560; time = 0.0214s; samplesPerSecond = 119380.7
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.19689636 * 2560; err = 0.36093750 * 2560; time = 0.0214s; samplesPerSecond = 119581.5
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.03015747 * 2560; err = 0.31992188 * 2560; time = 0.0214s; samplesPerSecond = 119369.6
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.08931274 * 2560; err = 0.33671875 * 2560; time = 0.0215s; samplesPerSecond = 119247.3
07/14/2016 07:51:31:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.07254944 * 2560; err = 0.34492187 * 2560; time = 0.0215s; samplesPerSecond = 119280.6
07/14/2016 07:51:31: Finished Epoch[ 2 of 2]: [Training] ce = 1.12036705 * 81920; err = 0.34296875 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.690176s
07/14/2016 07:51:31: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech'
07/14/2016 07:51:31: CNTKCommandTrainEnd: dptPre2

07/14/2016 07:51:31: Action "train" complete.


07/14/2016 07:51:31: ##############################################################################
07/14/2016 07:51:31: #                                                                            #
07/14/2016 07:51:31: # Action "edit"                                                              #
07/14/2016 07:51:31: #                                                                            #
07/14/2016 07:51:31: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/14/2016 07:51:31: Action "edit" complete.


07/14/2016 07:51:31: ##############################################################################
07/14/2016 07:51:31: #                                                                            #
07/14/2016 07:51:31: # Action "train"                                                             #
07/14/2016 07:51:31: #                                                                            #
07/14/2016 07:51:31: ##############################################################################

07/14/2016 07:51:31: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 946 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252508 frames in 946 out of 946 utterances
label set 0: 129 classes
minibatchutterancesource: 946 utterances grouped into 3 chunks, av. chunk size: 315.3 utterances, 84169.3 frames

07/14/2016 07:51:31: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/14/2016 07:51:32: Loaded model with 29 nodes on GPU 0.

07/14/2016 07:51:32: Training criterion node(s):
07/14/2016 07:51:32: 	ce = CrossEntropyWithSoftmax

07/14/2016 07:51:32: Evaluation criterion node(s):

07/14/2016 07:51:32: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *6]] [features Gradient[363 x *6]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *6]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *6]] }
00000007CA690BA0: {[HL3.W Value[512 x 512]] }
00000007D0C10740: {[globalInvStd Value[363 x 1]] }
00000007D0C10CE0: {[globalPrior Value[132 x 1]] }
00000007D0C10D80: {[HL1.b Value[512 x 1]] }
00000007D0C10F60: {[HL1.W Value[512 x 363]] }
00000007D0C11A00: {[features Value[363 x *6]] }
00000007D0C11C80: {[globalMean Value[363 x 1]] }
00000007D37C62C0: {[HL2.b Value[512 x 1]] }
00000007D37C6540: {[HL2.W Value[512 x 512]] }
00000007D37C6AE0: {[HL3.b Value[512 x 1]] }
00000007D66BFEC0: {[OL.b Value[132 x 1]] }
00000007D677ACC0: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *6]] }
00000007D677AFE0: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *6]] }
00000007D677B120: {[ce Value[1]] }
00000007D677B4E0: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *6]] }
00000007D677B6C0: {[OL.W Value[132 x 512]] }
00000007D677B760: {[featNorm Value[363 x *6]] }
00000007D677B8A0: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *6]] [OL.z Gradient[132 x 1 x *6]] }
00000007D677BB20: {[HL1.t Value[512 x *6]] }
00000007D677BC60: {[HL1.z Gradient[512 x 1 x *6]] [HL2.t Value[512 x 1 x *6]] }
00000007D677BD00: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *6]] }
00000007D677BDA0: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *6]] [HL2.z Gradient[512 x 1 x *6]] [HL3.t Value[512 x 1 x *6]] }
00000007D677BE40: {[HL3.t Gradient[512 x 1 x *6]] [HL3.y Value[512 x 1 x *6]] }
00000007D677BF80: {[scaledLogLikelihood Value[132 x 1 x *6]] }
00000007D677C020: {[err Value[1]] }
00000007D677C0C0: {[ce Gradient[1]] }
00000007D677C160: {[OL.b Gradient[132 x 1]] }
00000007D677C3E0: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *6]] [HL3.z Gradient[512 x 1 x *6]] [OL.t Value[132 x 1 x *6]] }
00000007D677C520: {[HL1.t Gradient[512 x *6]] [HL1.y Value[512 x 1 x *6]] }
00000007D677C700: {[logPrior Value[132 x 1]] }
00000007D677C980: {[OL.t Gradient[132 x 1 x *6]] }
00000007D677CA20: {[HL2.t Gradient[512 x 1 x *6]] [HL2.y Value[512 x 1 x *6]] }
00000007E527EE40: {[labels Value[132 x *6]] }

07/14/2016 07:51:32: No PreCompute nodes found, skipping PreCompute step.

07/14/2016 07:51:32: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/14/2016 07:51:32: Starting minibatch loop.
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.13%]: ce = 4.02675285 * 2560; err = 0.81171875 * 2560; time = 0.0309s; samplesPerSecond = 82939.2
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.61816978 * 2560; err = 0.64765625 * 2560; time = 0.0286s; samplesPerSecond = 89398.0
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 1.98039474 * 2560; err = 0.54179687 * 2560; time = 0.0287s; samplesPerSecond = 89124.1
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.73512497 * 2560; err = 0.48046875 * 2560; time = 0.0286s; samplesPerSecond = 89363.6
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.63%]: ce = 1.57313690 * 2560; err = 0.44843750 * 2560; time = 0.0286s; samplesPerSecond = 89413.6
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.51164322 * 2560; err = 0.44257812 * 2560; time = 0.0286s; samplesPerSecond = 89404.2
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.40619354 * 2560; err = 0.39609375 * 2560; time = 0.0287s; samplesPerSecond = 89220.4
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.36033020 * 2560; err = 0.38828125 * 2560; time = 0.0286s; samplesPerSecond = 89407.3
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.13%]: ce = 1.29484558 * 2560; err = 0.39335938 * 2560; time = 0.0287s; samplesPerSecond = 89320.0
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.32677155 * 2560; err = 0.39335938 * 2560; time = 0.0285s; samplesPerSecond = 89717.5
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.25594940 * 2560; err = 0.37500000 * 2560; time = 0.0286s; samplesPerSecond = 89501.1
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.20270844 * 2560; err = 0.35039063 * 2560; time = 0.0287s; samplesPerSecond = 89332.4
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.63%]: ce = 1.24975128 * 2560; err = 0.38085938 * 2560; time = 0.0286s; samplesPerSecond = 89385.5
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.23720093 * 2560; err = 0.37773438 * 2560; time = 0.0287s; samplesPerSecond = 89124.1
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.19117584 * 2560; err = 0.34726563 * 2560; time = 0.0286s; samplesPerSecond = 89498.0
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.22501373 * 2560; err = 0.35976562 * 2560; time = 0.0286s; samplesPerSecond = 89535.5
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.13%]: ce = 1.23222046 * 2560; err = 0.37578125 * 2560; time = 0.0287s; samplesPerSecond = 89245.3
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.18727722 * 2560; err = 0.35195312 * 2560; time = 0.0287s; samplesPerSecond = 89313.7
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.18016052 * 2560; err = 0.35585937 * 2560; time = 0.0287s; samplesPerSecond = 89288.8
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.16932983 * 2560; err = 0.34960938 * 2560; time = 0.0287s; samplesPerSecond = 89267.0
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.63%]: ce = 1.17571716 * 2560; err = 0.35273437 * 2560; time = 0.0287s; samplesPerSecond = 89158.2
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.21194153 * 2560; err = 0.35898438 * 2560; time = 0.0286s; samplesPerSecond = 89394.8
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.11492310 * 2560; err = 0.33398438 * 2560; time = 0.0286s; samplesPerSecond = 89532.4
07/14/2016 07:51:32:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.12993774 * 2560; err = 0.34375000 * 2560; time = 0.0287s; samplesPerSecond = 89220.4
07/14/2016 07:51:33:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.13%]: ce = 1.21492615 * 2560; err = 0.37539062 * 2560; time = 0.0287s; samplesPerSecond = 89074.5
07/14/2016 07:51:33:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.21964111 * 2560; err = 0.36953125 * 2560; time = 0.0286s; samplesPerSecond = 89472.9
07/14/2016 07:51:33:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.15922546 * 2560; err = 0.35078125 * 2560; time = 0.0286s; samplesPerSecond = 89369.9
07/14/2016 07:51:33:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.15700684 * 2560; err = 0.35351563 * 2560; time = 0.0288s; samplesPerSecond = 88793.3
07/14/2016 07:51:33:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.63%]: ce = 1.12997131 * 2560; err = 0.35156250 * 2560; time = 0.0287s; samplesPerSecond = 89229.7
07/14/2016 07:51:33:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.10640259 * 2560; err = 0.33984375 * 2560; time = 0.0288s; samplesPerSecond = 88956.8
07/14/2016 07:51:33:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.12984924 * 2560; err = 0.33945313 * 2560; time = 0.0287s; samplesPerSecond = 89254.6
07/14/2016 07:51:33:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.18217773 * 2560; err = 0.36171875 * 2560; time = 0.0286s; samplesPerSecond = 89376.1
07/14/2016 07:51:33: Finished Epoch[ 1 of 4]: [Training] ce = 1.40299597 * 81920; err = 0.39997559 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.09115s
07/14/2016 07:51:33: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.1'

07/14/2016 07:51:33: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/14/2016 07:51:33: Starting minibatch loop.
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.31322651 * 5120; err = 0.39121094 * 5120; time = 0.0444s; samplesPerSecond = 115232.3
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.23894253 * 5120; err = 0.37187500 * 5120; time = 0.0416s; samplesPerSecond = 122967.6
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.19464302 * 5120; err = 0.36484375 * 5120; time = 0.0417s; samplesPerSecond = 122652.4
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.14342537 * 5120; err = 0.35644531 * 5120; time = 0.0417s; samplesPerSecond = 122743.5
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.13800201 * 5120; err = 0.35253906 * 5120; time = 0.0417s; samplesPerSecond = 122796.5
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.12749023 * 5120; err = 0.34550781 * 5120; time = 0.0416s; samplesPerSecond = 123044.4
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.16824417 * 5120; err = 0.35429688 * 5120; time = 0.0418s; samplesPerSecond = 122602.4
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.11061783 * 5120; err = 0.35078125 * 5120; time = 0.0416s; samplesPerSecond = 123133.2
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.16170883 * 5120; err = 0.34941406 * 5120; time = 0.0417s; samplesPerSecond = 122899.7
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.10113449 * 5120; err = 0.34199219 * 5120; time = 0.0417s; samplesPerSecond = 122837.7
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.09601440 * 5120; err = 0.33593750 * 5120; time = 0.0416s; samplesPerSecond = 123029.6
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.06432037 * 5120; err = 0.32949219 * 5120; time = 0.0418s; samplesPerSecond = 122523.2
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.08236084 * 5120; err = 0.32792969 * 5120; time = 0.0417s; samplesPerSecond = 122917.4
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.05531616 * 5120; err = 0.33125000 * 5120; time = 0.0417s; samplesPerSecond = 122823.0
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.12295685 * 5120; err = 0.34765625 * 5120; time = 0.0417s; samplesPerSecond = 122690.6
07/14/2016 07:51:33:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.07241669 * 5120; err = 0.33496094 * 5120; time = 0.0417s; samplesPerSecond = 122670.0
07/14/2016 07:51:33: Finished Epoch[ 2 of 4]: [Training] ce = 1.13692627 * 81920; err = 0.34913330 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.672708s
07/14/2016 07:51:34: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.2'

07/14/2016 07:51:34: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

07/14/2016 07:51:34: Starting minibatch loop.
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.06326103 * 5120; err = 0.33574219 * 5120; time = 0.0423s; samplesPerSecond = 121063.1
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.06865788 * 5120; err = 0.32675781 * 5120; time = 0.0419s; samplesPerSecond = 122338.8
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.08199406 * 5120; err = 0.33906250 * 5120; time = 0.0418s; samplesPerSecond = 122365.1
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.11750526 * 5120; err = 0.34121094 * 5120; time = 0.0419s; samplesPerSecond = 122198.6
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.07665176 * 5120; err = 0.33300781 * 5120; time = 0.0417s; samplesPerSecond = 122661.2
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.06622810 * 5120; err = 0.33144531 * 5120; time = 0.0418s; samplesPerSecond = 122382.6
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.06092148 * 5120; err = 0.32363281 * 5120; time = 0.0417s; samplesPerSecond = 122787.7
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.03353500 * 5120; err = 0.31796875 * 5120; time = 0.0417s; samplesPerSecond = 122784.7
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.10433502 * 5120; err = 0.34101562 * 5120; time = 0.0418s; samplesPerSecond = 122620.0
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.10953445 * 5120; err = 0.33144531 * 5120; time = 0.0417s; samplesPerSecond = 122846.6
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.05407028 * 5120; err = 0.32773438 * 5120; time = 0.0417s; samplesPerSecond = 122637.7
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.02330170 * 5120; err = 0.31523438 * 5120; time = 0.0419s; samplesPerSecond = 122289.1
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.03003387 * 5120; err = 0.31640625 * 5120; time = 0.0418s; samplesPerSecond = 122502.7
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.03139343 * 5120; err = 0.32128906 * 5120; time = 0.0417s; samplesPerSecond = 122655.3
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.04165497 * 5120; err = 0.32382813 * 5120; time = 0.0417s; samplesPerSecond = 122873.1
07/14/2016 07:51:34:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.02103119 * 5120; err = 0.32207031 * 5120; time = 0.0418s; samplesPerSecond = 122479.2
07/14/2016 07:51:34: Finished Epoch[ 3 of 4]: [Training] ce = 1.06150684 * 81920; err = 0.32799072 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.671757s
07/14/2016 07:51:34: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.3'

07/14/2016 07:51:34: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

07/14/2016 07:51:34: Starting minibatch loop.
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.01175184 * 5120; err = 0.31757812 * 5120; time = 0.0423s; samplesPerSecond = 121140.4
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 0.99360217 * 4700; err = 0.30957447 * 4700; time = 0.1028s; samplesPerSecond = 45715.4
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.02129631 * 5120; err = 0.31093750 * 5120; time = 0.0417s; samplesPerSecond = 122787.7
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 0.98951569 * 5120; err = 0.31386719 * 5120; time = 0.0417s; samplesPerSecond = 122758.2
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.01124077 * 5120; err = 0.31503906 * 5120; time = 0.0418s; samplesPerSecond = 122567.2
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.00471077 * 5120; err = 0.31269531 * 5120; time = 0.0417s; samplesPerSecond = 122643.5
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.04003067 * 5120; err = 0.32636719 * 5120; time = 0.0417s; samplesPerSecond = 122917.4
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.01847687 * 5120; err = 0.31386719 * 5120; time = 0.0417s; samplesPerSecond = 122728.8
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.02150726 * 5120; err = 0.32695313 * 5120; time = 0.0417s; samplesPerSecond = 122808.3
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.00081406 * 5120; err = 0.31113281 * 5120; time = 0.0415s; samplesPerSecond = 123325.9
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.01322250 * 5120; err = 0.31054688 * 5120; time = 0.0417s; samplesPerSecond = 122852.5
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 0.97127304 * 5120; err = 0.29687500 * 5120; time = 0.0418s; samplesPerSecond = 122517.3
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.03328323 * 5120; err = 0.31953125 * 5120; time = 0.0416s; samplesPerSecond = 122949.8
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.97039032 * 5120; err = 0.29453125 * 5120; time = 0.0417s; samplesPerSecond = 122852.5
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.97466125 * 5120; err = 0.30351563 * 5120; time = 0.0417s; samplesPerSecond = 122749.4
07/14/2016 07:51:35:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.02392120 * 5120; err = 0.31484375 * 5120; time = 0.0417s; samplesPerSecond = 122828.9
07/14/2016 07:51:35: Finished Epoch[ 4 of 4]: [Training] ce = 1.00590687 * 81920; err = 0.31236572 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.735989s
07/14/2016 07:51:35: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714075121.487046\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech'
07/14/2016 07:51:35: CNTKCommandTrainEnd: speechTrain

07/14/2016 07:51:35: Action "train" complete.

07/14/2016 07:51:35: __COMPLETED__