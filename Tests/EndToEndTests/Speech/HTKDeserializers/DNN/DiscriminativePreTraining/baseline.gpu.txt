CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 264172964 kB
-------------------------------------------------------------------
=== Running /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.cntk currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data RunDir=/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining OutputDir=/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 13:57:01
		Last modified date: Thu Jul 14 12:50:48 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: b918e06de12613a1773a0976ab94e213bd09ce52
		Built by philly on adf92da755f9
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
07/14/2016 14:13:23: -------------------------------------------------------------------
07/14/2016 14:13:23: Build info: 

07/14/2016 14:13:23: 		Built time: Jul 14 2016 13:57:01
07/14/2016 14:13:23: 		Last modified date: Thu Jul 14 12:50:48 2016
07/14/2016 14:13:23: 		Build type: release
07/14/2016 14:13:23: 		Build target: GPU
07/14/2016 14:13:23: 		With 1bit-SGD: no
07/14/2016 14:13:23: 		Math lib: mkl
07/14/2016 14:13:23: 		CUDA_PATH: /usr/local/cuda-7.5
07/14/2016 14:13:23: 		CUB_PATH: /usr/local/cub-1.4.1
07/14/2016 14:13:23: 		CUDNN_PATH: /usr/local/cudnn-4.0
07/14/2016 14:13:23: 		Build Branch: HEAD
07/14/2016 14:13:23: 		Build SHA1: b918e06de12613a1773a0976ab94e213bd09ce52
07/14/2016 14:13:23: 		Built by philly on adf92da755f9
07/14/2016 14:13:23: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
07/14/2016 14:13:23: -------------------------------------------------------------------
07/14/2016 14:13:24: -------------------------------------------------------------------
07/14/2016 14:13:24: GPU info:

07/14/2016 14:13:24: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/14/2016 14:13:24: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/14/2016 14:13:24: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/14/2016 14:13:24: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/14/2016 14:13:24: -------------------------------------------------------------------

07/14/2016 14:13:24: Running on localhost at 2016/07/14 14:13:24
07/14/2016 14:13:24: Command line: 
/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.cntk  currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  RunDir=/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu  DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining  OutputDir=/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu  DeviceId=0  timestamping=true



07/14/2016 14:13:24: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
07/14/2016 14:13:24: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining
OutputDir=/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true

07/14/2016 14:13:24: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

07/14/2016 14:13:24: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
07/14/2016 14:13:24: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining
OutputDir=/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true

07/14/2016 14:13:24: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

07/14/2016 14:13:24: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining
configparameters: cntk_dpt.cntk:currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk_dpt.cntk:DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk_dpt.cntk:deviceId=0
configparameters: cntk_dpt.cntk:dptPre1=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:dptPre2=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.cntk:ndlMacros=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.cntk:OutputDir=/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:precision=float
configparameters: cntk_dpt.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_dpt.cntk:RunDir=/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.cntk:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.cntk:timestamping=true
configparameters: cntk_dpt.cntk:traceLevel=1
07/14/2016 14:13:24: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
07/14/2016 14:13:24: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain
07/14/2016 14:13:24: Precision = "float"
07/14/2016 14:13:24: CNTKModelPath: /tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech
07/14/2016 14:13:24: CNTKCommandTrainInfo: dptPre1 : 2
07/14/2016 14:13:24: CNTKModelPath: /tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech
07/14/2016 14:13:24: CNTKCommandTrainInfo: dptPre2 : 2
07/14/2016 14:13:24: CNTKModelPath: /tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech
07/14/2016 14:13:24: CNTKCommandTrainInfo: speechTrain : 4
07/14/2016 14:13:24: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8

07/14/2016 14:13:24: ##############################################################################
07/14/2016 14:13:24: #                                                                            #
07/14/2016 14:13:24: # Action "train"                                                             #
07/14/2016 14:13:24: #                                                                            #
07/14/2016 14:13:24: ##############################################################################

07/14/2016 14:13:24: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 946 entries
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252508 frames in 946 out of 946 utterances
label set 0: 129 classes
minibatchutterancesource: 946 utterances grouped into 3 chunks, av. chunk size: 315.3 utterances, 84169.3 frames

07/14/2016 14:13:24: Creating virgin network.
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/14/2016 14:13:24: Created model with 19 nodes on GPU 0.

07/14/2016 14:13:24: Training criterion node(s):
07/14/2016 14:13:24: 	ce = CrossEntropyWithSoftmax

07/14/2016 14:13:24: Evaluation criterion node(s):

07/14/2016 14:13:24: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *]] [features Gradient[363 x *]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *]] }
0x7f3a6d9bccf8: {[logPrior Value[132 x 1]] }
0x7f3a6d9bdfc8: {[featNorm Value[363 x *]] }
0x7f3a6d9be388: {[HL1.t Value[512 x *]] }
0x7f3a6d9be738: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *]] }
0x7f3a6d9be8f8: {[HL1.t Gradient[512 x *]] [HL1.y Value[512 x 1 x *]] }
0x7f3a6d9beab8: {[HL1.z Gradient[512 x 1 x *]] [OL.t Value[132 x 1 x *]] }
0x7f3a6d9bec78: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *]] }
0x7f3a6d9bf6f8: {[ce Gradient[1]] }
0x7f3a6d9bf8b8: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *]] [OL.z Gradient[132 x 1 x *]] }
0x7f3a6d9bfa78: {[OL.t Gradient[132 x 1 x *]] }
0x7f3a6d9bfc38: {[OL.b Gradient[132 x 1]] }
0x7f3a6e2a5ff8: {[labels Value[132 x *]] }
0x7f3a6e2a6bb8: {[globalInvStd Value[363 x 1]] }
0x7f3a6e2a6eb8: {[globalMean Value[363 x 1]] }
0x7f3a6e2a81e8: {[globalPrior Value[132 x 1]] }
0x7f3a6e2a8bc8: {[HL1.W Value[512 x 363]] }
0x7f3a6e2aa0c8: {[HL1.b Value[512 x 1]] }
0x7f3a6e2ab378: {[OL.W Value[132 x 512]] }
0x7f3a6e2abb78: {[OL.b Value[132 x 1]] }
0x7f3a73237878: {[err Value[1]] }
0x7f3a73237a38: {[scaledLogLikelihood Value[132 x 1 x *]] }
0x7f3a73237bf8: {[ce Value[1]] }
0x7f3a73241458: {[features Value[363 x *]] }

07/14/2016 14:13:24: No PreCompute nodes found, skipping PreCompute step.

07/14/2016 14:13:24: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/14/2016 14:13:24: Starting minibatch loop.
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 3.78373604 * 2560; err = 0.83632812 * 2560; time = 0.1170s; samplesPerSecond = 21879.0
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.86919212 * 2560; err = 0.70507812 * 2560; time = 0.0086s; samplesPerSecond = 298821.1
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.44880524 * 2560; err = 0.61406250 * 2560; time = 0.0085s; samplesPerSecond = 300082.1
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.16399231 * 2560; err = 0.56093750 * 2560; time = 0.0085s; samplesPerSecond = 301247.4
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 2.04136200 * 2560; err = 0.56132812 * 2560; time = 0.0085s; samplesPerSecond = 301531.2
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.87986603 * 2560; err = 0.52500000 * 2560; time = 0.0085s; samplesPerSecond = 302386.0
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.80020599 * 2560; err = 0.52109375 * 2560; time = 0.0085s; samplesPerSecond = 302064.9
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.73899231 * 2560; err = 0.49453125 * 2560; time = 0.0085s; samplesPerSecond = 302493.2
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.62734528 * 2560; err = 0.47031250 * 2560; time = 0.0084s; samplesPerSecond = 302994.4
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.59208374 * 2560; err = 0.45312500 * 2560; time = 0.0084s; samplesPerSecond = 303425.4
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.59369659 * 2560; err = 0.46250000 * 2560; time = 0.0085s; samplesPerSecond = 301282.8
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.49622498 * 2560; err = 0.43242188 * 2560; time = 0.0085s; samplesPerSecond = 302136.2
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.46433868 * 2560; err = 0.43281250 * 2560; time = 0.0085s; samplesPerSecond = 302386.0
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.49246826 * 2560; err = 0.42851563 * 2560; time = 0.0085s; samplesPerSecond = 301958.0
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.45256958 * 2560; err = 0.42421875 * 2560; time = 0.0085s; samplesPerSecond = 302886.9
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.39721985 * 2560; err = 0.42421875 * 2560; time = 0.0085s; samplesPerSecond = 302100.5
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.44670715 * 2560; err = 0.43515625 * 2560; time = 0.0085s; samplesPerSecond = 302636.2
07/14/2016 14:13:24:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.42720032 * 2560; err = 0.43125000 * 2560; time = 0.0085s; samplesPerSecond = 301282.8
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.33662415 * 2560; err = 0.40234375 * 2560; time = 0.0084s; samplesPerSecond = 302994.4
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.37653503 * 2560; err = 0.40312500 * 2560; time = 0.0085s; samplesPerSecond = 301744.5
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.29439392 * 2560; err = 0.39062500 * 2560; time = 0.0085s; samplesPerSecond = 302493.2
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.35942993 * 2560; err = 0.39140625 * 2560; time = 0.0084s; samplesPerSecond = 303461.4
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.27086487 * 2560; err = 0.38554688 * 2560; time = 0.0089s; samplesPerSecond = 287156.5
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.29682922 * 2560; err = 0.39804688 * 2560; time = 0.0078s; samplesPerSecond = 327073.0
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.28977966 * 2560; err = 0.39062500 * 2560; time = 0.0079s; samplesPerSecond = 324667.1
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.23562622 * 2560; err = 0.37851563 * 2560; time = 0.0079s; samplesPerSecond = 323722.8
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.26679993 * 2560; err = 0.36835937 * 2560; time = 0.0079s; samplesPerSecond = 322784.0
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.28395996 * 2560; err = 0.38906250 * 2560; time = 0.0078s; samplesPerSecond = 326197.8
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.29333496 * 2560; err = 0.39140625 * 2560; time = 0.0079s; samplesPerSecond = 324584.8
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.26304932 * 2560; err = 0.37539062 * 2560; time = 0.0079s; samplesPerSecond = 324667.1
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.24609985 * 2560; err = 0.37695312 * 2560; time = 0.0079s; samplesPerSecond = 324584.8
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.25902100 * 2560; err = 0.38085938 * 2560; time = 0.0079s; samplesPerSecond = 324543.6
07/14/2016 14:13:25: Finished Epoch[ 1 of 2]: [Training] ce = 1.61838608 * 81920; err = 0.45734863 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.461864s
07/14/2016 14:13:25: SGD: Saving checkpoint model '/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech.1'

07/14/2016 14:13:25: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/14/2016 14:13:25: Starting minibatch loop.
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.24799643 * 2560; err = 0.37617187 * 2560; time = 0.0091s; samplesPerSecond = 280671.0
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.25826807 * 2560; err = 0.36953125 * 2560; time = 0.0079s; samplesPerSecond = 323518.3
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.24438324 * 2560; err = 0.37968750 * 2560; time = 0.0080s; samplesPerSecond = 320400.5
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.25809441 * 2560; err = 0.38007812 * 2560; time = 0.0080s; samplesPerSecond = 321365.8
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.21047211 * 2560; err = 0.36992188 * 2560; time = 0.0080s; samplesPerSecond = 320400.5
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.19904137 * 2560; err = 0.35976562 * 2560; time = 0.0080s; samplesPerSecond = 319880.0
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.24886093 * 2560; err = 0.38437500 * 2560; time = 0.0079s; samplesPerSecond = 323395.7
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.21887894 * 2560; err = 0.37226562 * 2560; time = 0.0079s; samplesPerSecond = 322824.7
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.23372650 * 2560; err = 0.36328125 * 2560; time = 0.0085s; samplesPerSecond = 300011.7
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.18381042 * 2560; err = 0.36250000 * 2560; time = 0.0079s; samplesPerSecond = 322702.6
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.26340485 * 2560; err = 0.40039062 * 2560; time = 0.0079s; samplesPerSecond = 322377.5
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.16100616 * 2560; err = 0.36093750 * 2560; time = 0.0079s; samplesPerSecond = 325409.9
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.13385162 * 2560; err = 0.34648438 * 2560; time = 0.0079s; samplesPerSecond = 322784.0
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.16377716 * 2560; err = 0.35781250 * 2560; time = 0.0078s; samplesPerSecond = 327659.0
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.18569183 * 2560; err = 0.35429688 * 2560; time = 0.0078s; samplesPerSecond = 326530.6
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.11697845 * 2560; err = 0.34140625 * 2560; time = 0.0079s; samplesPerSecond = 325699.7
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.15262451 * 2560; err = 0.35117188 * 2560; time = 0.0079s; samplesPerSecond = 325038.1
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.15663300 * 2560; err = 0.36093750 * 2560; time = 0.0078s; samplesPerSecond = 328331.4
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.14303589 * 2560; err = 0.35156250 * 2560; time = 0.0079s; samplesPerSecond = 325120.7
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.17419281 * 2560; err = 0.35117188 * 2560; time = 0.0079s; samplesPerSecond = 325948.6
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.19798126 * 2560; err = 0.37578125 * 2560; time = 0.0079s; samplesPerSecond = 325492.7
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.14432983 * 2560; err = 0.35351562 * 2560; time = 0.0079s; samplesPerSecond = 325948.6
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.14004211 * 2560; err = 0.36484375 * 2560; time = 0.0077s; samplesPerSecond = 330663.9
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.12702332 * 2560; err = 0.35703125 * 2560; time = 0.0077s; samplesPerSecond = 331434.5
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.13106995 * 2560; err = 0.33906250 * 2560; time = 0.0077s; samplesPerSecond = 331177.2
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.12968140 * 2560; err = 0.34140625 * 2560; time = 0.0078s; samplesPerSecond = 329429.9
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.15039978 * 2560; err = 0.34375000 * 2560; time = 0.0077s; samplesPerSecond = 331048.8
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.19716492 * 2560; err = 0.37226562 * 2560; time = 0.0078s; samplesPerSecond = 329091.1
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.18566284 * 2560; err = 0.35859375 * 2560; time = 0.0078s; samplesPerSecond = 327365.7
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.10610352 * 2560; err = 0.34804687 * 2560; time = 0.0078s; samplesPerSecond = 328036.9
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.11772766 * 2560; err = 0.34023437 * 2560; time = 0.0078s; samplesPerSecond = 328837.5
07/14/2016 14:13:25:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.12360840 * 2560; err = 0.35351562 * 2560; time = 0.0077s; samplesPerSecond = 330792.1
07/14/2016 14:13:25: Finished Epoch[ 2 of 2]: [Training] ce = 1.17829762 * 81920; err = 0.36068115 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.256446s
07/14/2016 14:13:25: SGD: Saving checkpoint model '/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech'
07/14/2016 14:13:25: CNTKCommandTrainEnd: dptPre1

07/14/2016 14:13:25: Action "train" complete.


07/14/2016 14:13:25: ##############################################################################
07/14/2016 14:13:25: #                                                                            #
07/14/2016 14:13:25: # Action "edit"                                                              #
07/14/2016 14:13:25: #                                                                            #
07/14/2016 14:13:25: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/14/2016 14:13:25: Action "edit" complete.


07/14/2016 14:13:25: ##############################################################################
07/14/2016 14:13:25: #                                                                            #
07/14/2016 14:13:25: # Action "train"                                                             #
07/14/2016 14:13:25: #                                                                            #
07/14/2016 14:13:25: ##############################################################################

07/14/2016 14:13:25: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 946 entries
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252508 frames in 946 out of 946 utterances
label set 0: 129 classes
minibatchutterancesource: 946 utterances grouped into 3 chunks, av. chunk size: 315.3 utterances, 84169.3 frames

07/14/2016 14:13:25: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/14/2016 14:13:25: Loaded model with 24 nodes on GPU 0.

07/14/2016 14:13:25: Training criterion node(s):
07/14/2016 14:13:25: 	ce = CrossEntropyWithSoftmax

07/14/2016 14:13:25: Evaluation criterion node(s):

07/14/2016 14:13:25: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *3]] [features Gradient[363 x *3]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *3]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *3]] }
0x7f3a63a26bc8: {[labels Value[132 x *3]] }
0x7f3a63a43498: {[featNorm Value[363 x *3]] }
0x7f3a63a77d38: {[globalPrior Value[132 x 1]] }
0x7f3a63a780a8: {[err Value[1]] }
0x7f3a63a8c448: {[ce Gradient[1]] }
0x7f3a63a8fc48: {[HL1.W Value[512 x 363]] }
0x7f3a63a9a038: {[scaledLogLikelihood Value[132 x 1 x *3]] }
0x7f3a63a9a1f8: {[ce Value[1]] }
0x7f3a6d9c9a58: {[HL1.t Gradient[512 x *3]] [HL1.y Value[512 x 1 x *3]] }
0x7f3a6d9c9c18: {[HL1.z Gradient[512 x 1 x *3]] [HL2.t Value[512 x 1 x *3]] }
0x7f3a6d9c9dd8: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *3]] }
0x7f3a6d9c9f98: {[HL2.t Gradient[512 x 1 x *3]] [HL2.y Value[512 x 1 x *3]] }
0x7f3a7203b578: {[globalInvStd Value[363 x 1]] }
0x7f3a720bf4a8: {[OL.W Value[132 x 512]] }
0x7f3a73202968: {[HL2.W Value[512 x 512]] }
0x7f3a7321f9b8: {[globalMean Value[363 x 1]] }
0x7f3a73237748: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *3]] [OL.z Gradient[132 x 1 x *3]] }
0x7f3a73237908: {[OL.t Gradient[132 x 1 x *3]] }
0x7f3a73237ac8: {[OL.b Gradient[132 x 1]] }
0x7f3a73245238: {[HL1.b Value[512 x 1]] }
0x7f3a73246ad8: {[HL2.b Value[512 x 1]] }
0x7f3a73276188: {[OL.b Value[132 x 1]] }
0x7f3a732a4ea8: {[features Value[363 x *3]] }
0x7f3a732c0c68: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *3]] [HL2.z Gradient[512 x 1 x *3]] [OL.t Value[132 x 1 x *3]] }
0x7f3a732c0e28: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *3]] }
0x7f3a732c5208: {[HL1.t Value[512 x *3]] }
0x7f3a732c5418: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *3]] }
0x7f3a732ef068: {[logPrior Value[132 x 1]] }

07/14/2016 14:13:25: No PreCompute nodes found, skipping PreCompute step.

07/14/2016 14:13:25: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/14/2016 14:13:25: Starting minibatch loop.
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 4.85880775 * 2560; err = 0.81835938 * 2560; time = 0.0149s; samplesPerSecond = 171708.4
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.88092384 * 2560; err = 0.70273438 * 2560; time = 0.0132s; samplesPerSecond = 193983.5
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.34660797 * 2560; err = 0.61914062 * 2560; time = 0.0123s; samplesPerSecond = 208248.6
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.99803314 * 2560; err = 0.55781250 * 2560; time = 0.0122s; samplesPerSecond = 209355.6
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.79064560 * 2560; err = 0.50664062 * 2560; time = 0.0122s; samplesPerSecond = 209578.4
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.64251709 * 2560; err = 0.47695312 * 2560; time = 0.0122s; samplesPerSecond = 209629.9
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.56817322 * 2560; err = 0.44960937 * 2560; time = 0.0122s; samplesPerSecond = 210249.7
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.48361511 * 2560; err = 0.43281250 * 2560; time = 0.0122s; samplesPerSecond = 209784.5
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.43886414 * 2560; err = 0.42539063 * 2560; time = 0.0122s; samplesPerSecond = 210336.0
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.43141327 * 2560; err = 0.42382812 * 2560; time = 0.0122s; samplesPerSecond = 210422.5
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.40253906 * 2560; err = 0.41289063 * 2560; time = 0.0122s; samplesPerSecond = 209801.7
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.37527466 * 2560; err = 0.41054687 * 2560; time = 0.0123s; samplesPerSecond = 208945.5
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.36898346 * 2560; err = 0.41484375 * 2560; time = 0.0122s; samplesPerSecond = 210387.9
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.38811035 * 2560; err = 0.41445312 * 2560; time = 0.0122s; samplesPerSecond = 209235.8
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.35626221 * 2560; err = 0.40507813 * 2560; time = 0.0122s; samplesPerSecond = 210059.9
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.29679871 * 2560; err = 0.39335938 * 2560; time = 0.0122s; samplesPerSecond = 210578.3
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.32161255 * 2560; err = 0.39687500 * 2560; time = 0.0122s; samplesPerSecond = 210370.6
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.28822021 * 2560; err = 0.39882812 * 2560; time = 0.0122s; samplesPerSecond = 209629.9
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.22451172 * 2560; err = 0.36093750 * 2560; time = 0.0122s; samplesPerSecond = 210059.9
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.28240051 * 2560; err = 0.39140625 * 2560; time = 0.0122s; samplesPerSecond = 209235.8
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.21960754 * 2560; err = 0.36796875 * 2560; time = 0.0122s; samplesPerSecond = 209647.0
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.28277893 * 2560; err = 0.37890625 * 2560; time = 0.0122s; samplesPerSecond = 209887.7
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.16069336 * 2560; err = 0.34921875 * 2560; time = 0.0122s; samplesPerSecond = 209664.2
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.20777283 * 2560; err = 0.37617187 * 2560; time = 0.0122s; samplesPerSecond = 209939.3
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.20460510 * 2560; err = 0.36171875 * 2560; time = 0.0122s; samplesPerSecond = 209956.5
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.15525513 * 2560; err = 0.35507813 * 2560; time = 0.0122s; samplesPerSecond = 209836.1
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.21582031 * 2560; err = 0.35742188 * 2560; time = 0.0122s; samplesPerSecond = 210336.0
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.17653198 * 2560; err = 0.35078125 * 2560; time = 0.0122s; samplesPerSecond = 209561.2
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.19760742 * 2560; err = 0.36289063 * 2560; time = 0.0121s; samplesPerSecond = 211064.4
07/14/2016 14:13:25:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.20890808 * 2560; err = 0.36210938 * 2560; time = 0.0122s; samplesPerSecond = 210560.9
07/14/2016 14:13:26:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.18209229 * 2560; err = 0.35585937 * 2560; time = 0.0122s; samplesPerSecond = 210215.1
07/14/2016 14:13:26:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.22672729 * 2560; err = 0.36953125 * 2560; time = 0.0122s; samplesPerSecond = 210699.6
07/14/2016 14:13:26: Finished Epoch[ 1 of 2]: [Training] ce = 1.53695984 * 81920; err = 0.43000488 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.480358s
07/14/2016 14:13:26: SGD: Saving checkpoint model '/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.1'

07/14/2016 14:13:26: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/14/2016 14:13:26: Starting minibatch loop.
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.22853079 * 2560; err = 0.37226562 * 2560; time = 0.0137s; samplesPerSecond = 187107.1
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.23235846 * 2560; err = 0.37031250 * 2560; time = 0.0123s; samplesPerSecond = 208418.1
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.19152470 * 2560; err = 0.36367187 * 2560; time = 0.0123s; samplesPerSecond = 208350.3
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.15000076 * 2560; err = 0.35468750 * 2560; time = 0.0122s; samplesPerSecond = 209064.9
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.13855095 * 2560; err = 0.35703125 * 2560; time = 0.0123s; samplesPerSecond = 208554.0
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.15206261 * 2560; err = 0.34101562 * 2560; time = 0.0123s; samplesPerSecond = 208316.4
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.16194687 * 2560; err = 0.35312500 * 2560; time = 0.0131s; samplesPerSecond = 195973.4
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.18449631 * 2560; err = 0.36523438 * 2560; time = 0.0122s; samplesPerSecond = 209116.2
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.17826538 * 2560; err = 0.35507813 * 2560; time = 0.0123s; samplesPerSecond = 208792.1
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.14180374 * 2560; err = 0.34921875 * 2560; time = 0.0122s; samplesPerSecond = 209030.8
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.19046555 * 2560; err = 0.37070313 * 2560; time = 0.0123s; samplesPerSecond = 208945.5
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.14977875 * 2560; err = 0.35781250 * 2560; time = 0.0122s; samplesPerSecond = 209064.9
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.10634918 * 2560; err = 0.33710937 * 2560; time = 0.0123s; samplesPerSecond = 208197.8
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.13144379 * 2560; err = 0.35312500 * 2560; time = 0.0123s; samplesPerSecond = 208860.2
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.13214111 * 2560; err = 0.34296875 * 2560; time = 0.0122s; samplesPerSecond = 209201.6
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.07518158 * 2560; err = 0.33007812 * 2560; time = 0.0123s; samplesPerSecond = 208809.1
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.09919434 * 2560; err = 0.33632812 * 2560; time = 0.0123s; samplesPerSecond = 208843.2
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.10649872 * 2560; err = 0.33750000 * 2560; time = 0.0123s; samplesPerSecond = 208758.1
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.10485382 * 2560; err = 0.33789062 * 2560; time = 0.0123s; samplesPerSecond = 208622.0
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.12979584 * 2560; err = 0.33906250 * 2560; time = 0.0123s; samplesPerSecond = 207438.6
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.12662964 * 2560; err = 0.35234375 * 2560; time = 0.0124s; samplesPerSecond = 206119.2
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.09451141 * 2560; err = 0.33828125 * 2560; time = 0.0123s; samplesPerSecond = 207590.0
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.09305420 * 2560; err = 0.34765625 * 2560; time = 0.0123s; samplesPerSecond = 207539.5
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.07683411 * 2560; err = 0.34023437 * 2560; time = 0.0123s; samplesPerSecond = 207438.6
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.07086792 * 2560; err = 0.33046875 * 2560; time = 0.0124s; samplesPerSecond = 206902.1
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.09484863 * 2560; err = 0.32890625 * 2560; time = 0.0124s; samplesPerSecond = 206885.4
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.07869263 * 2560; err = 0.32929687 * 2560; time = 0.0124s; samplesPerSecond = 206584.9
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.05427856 * 2560; err = 0.32929687 * 2560; time = 0.0124s; samplesPerSecond = 206584.9
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.08741760 * 2560; err = 0.33515625 * 2560; time = 0.0124s; samplesPerSecond = 206468.3
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.04409485 * 2560; err = 0.32539062 * 2560; time = 0.0123s; samplesPerSecond = 207405.0
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.06804199 * 2560; err = 0.33085938 * 2560; time = 0.0124s; samplesPerSecond = 207170.0
07/14/2016 14:13:26:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.07609253 * 2560; err = 0.32656250 * 2560; time = 0.0123s; samplesPerSecond = 207606.8
07/14/2016 14:13:26: Finished Epoch[ 2 of 2]: [Training] ce = 1.12345648 * 81920; err = 0.34495850 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.399202s
07/14/2016 14:13:26: SGD: Saving checkpoint model '/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech'
07/14/2016 14:13:26: CNTKCommandTrainEnd: dptPre2

07/14/2016 14:13:26: Action "train" complete.


07/14/2016 14:13:26: ##############################################################################
07/14/2016 14:13:26: #                                                                            #
07/14/2016 14:13:26: # Action "edit"                                                              #
07/14/2016 14:13:26: #                                                                            #
07/14/2016 14:13:26: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/14/2016 14:13:26: Action "edit" complete.


07/14/2016 14:13:26: ##############################################################################
07/14/2016 14:13:26: #                                                                            #
07/14/2016 14:13:26: # Action "train"                                                             #
07/14/2016 14:13:26: #                                                                            #
07/14/2016 14:13:26: ##############################################################################

07/14/2016 14:13:26: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 946 entries
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252508 frames in 946 out of 946 utterances
label set 0: 129 classes
minibatchutterancesource: 946 utterances grouped into 3 chunks, av. chunk size: 315.3 utterances, 84169.3 frames

07/14/2016 14:13:26: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/14/2016 14:13:26: Loaded model with 29 nodes on GPU 0.

07/14/2016 14:13:26: Training criterion node(s):
07/14/2016 14:13:26: 	ce = CrossEntropyWithSoftmax

07/14/2016 14:13:26: Evaluation criterion node(s):

07/14/2016 14:13:26: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *6]] [features Gradient[363 x *6]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *6]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *6]] }
0x7f3a63a127e8: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *6]] }
0x7f3a63a12c58: {[HL1.t Gradient[512 x *6]] [HL1.y Value[512 x 1 x *6]] }
0x7f3a63a12db8: {[HL1.z Gradient[512 x 1 x *6]] [HL2.t Value[512 x 1 x *6]] }
0x7f3a63a12f78: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *6]] }
0x7f3a63a13408: {[HL1.b Value[512 x 1]] }
0x7f3a63a548b8: {[OL.b Value[132 x 1]] }
0x7f3a63a54ab8: {[OL.W Value[132 x 512]] }
0x7f3a63a96428: {[HL3.b Value[512 x 1]] }
0x7f3a63a98f28: {[HL1.W Value[512 x 363]] }
0x7f3a63a99148: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *6]] }
0x7f3a63a9afb8: {[HL2.b Value[512 x 1]] }
0x7f3a63a9f428: {[err Value[1]] }
0x7f3a63abf438: {[globalMean Value[363 x 1]] }
0x7f3a63c6e828: {[labels Value[132 x *6]] }
0x7f3a63c6ea28: {[features Value[363 x *6]] }
0x7f3a6d9bd8e8: {[globalInvStd Value[363 x 1]] }
0x7f3a6d9c0bc8: {[scaledLogLikelihood Value[132 x 1 x *6]] }
0x7f3a6e2a58b8: {[ce Gradient[1]] }
0x7f3a6e2a5a78: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *6]] [OL.z Gradient[132 x 1 x *6]] }
0x7f3a6e2a5c38: {[OL.t Gradient[132 x 1 x *6]] }
0x7f3a6e2a5df8: {[OL.b Gradient[132 x 1]] }
0x7f3a73277198: {[ce Value[1]] }
0x7f3a732775b8: {[featNorm Value[363 x *6]] }
0x7f3a73283db8: {[HL2.t Gradient[512 x 1 x *6]] [HL2.y Value[512 x 1 x *6]] }
0x7f3a73283f78: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *6]] [HL2.z Gradient[512 x 1 x *6]] [HL3.t Value[512 x 1 x *6]] }
0x7f3a73284138: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *6]] }
0x7f3a732842f8: {[HL3.t Gradient[512 x 1 x *6]] [HL3.y Value[512 x 1 x *6]] }
0x7f3a732844b8: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *6]] [HL3.z Gradient[512 x 1 x *6]] [OL.t Value[132 x 1 x *6]] }
0x7f3a73298058: {[HL1.t Value[512 x *6]] }
0x7f3a73298218: {[logPrior Value[132 x 1]] }
0x7f3a73298c48: {[globalPrior Value[132 x 1]] }
0x7f3a732992f8: {[HL3.W Value[512 x 512]] }
0x7f3a732c1048: {[HL2.W Value[512 x 512]] }

07/14/2016 14:13:26: No PreCompute nodes found, skipping PreCompute step.

07/14/2016 14:13:26: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/14/2016 14:13:26: Starting minibatch loop.
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: ce = 3.96956596 * 2560; err = 0.80781250 * 2560; time = 0.0193s; samplesPerSecond = 132615.0
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.47116127 * 2560; err = 0.62304688 * 2560; time = 0.0166s; samplesPerSecond = 153772.2
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 1.96716843 * 2560; err = 0.53398437 * 2560; time = 0.0166s; samplesPerSecond = 153763.0
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.70347443 * 2560; err = 0.46757813 * 2560; time = 0.0166s; samplesPerSecond = 154216.9
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: ce = 1.57613678 * 2560; err = 0.44218750 * 2560; time = 0.0166s; samplesPerSecond = 154263.3
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.43280106 * 2560; err = 0.41289063 * 2560; time = 0.0166s; samplesPerSecond = 154207.6
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.39631348 * 2560; err = 0.40390625 * 2560; time = 0.0166s; samplesPerSecond = 153994.2
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.35779724 * 2560; err = 0.40664062 * 2560; time = 0.0166s; samplesPerSecond = 154412.2
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: ce = 1.30656891 * 2560; err = 0.38164063 * 2560; time = 0.0167s; samplesPerSecond = 153412.8
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.30930939 * 2560; err = 0.38906250 * 2560; time = 0.0166s; samplesPerSecond = 154040.6
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.30440674 * 2560; err = 0.38359375 * 2560; time = 0.0166s; samplesPerSecond = 153966.4
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.27718353 * 2560; err = 0.37929687 * 2560; time = 0.0167s; samplesPerSecond = 153183.3
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: ce = 1.23746033 * 2560; err = 0.37539062 * 2560; time = 0.0167s; samplesPerSecond = 153504.8
07/14/2016 14:13:26:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.28712158 * 2560; err = 0.38007812 * 2560; time = 0.0167s; samplesPerSecond = 153541.7
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.26145020 * 2560; err = 0.37968750 * 2560; time = 0.0167s; samplesPerSecond = 153744.5
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.19994812 * 2560; err = 0.36523438 * 2560; time = 0.0166s; samplesPerSecond = 154096.2
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: ce = 1.23166504 * 2560; err = 0.37734375 * 2560; time = 0.0166s; samplesPerSecond = 153910.9
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.20690613 * 2560; err = 0.36953125 * 2560; time = 0.0166s; samplesPerSecond = 154216.9
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.14271851 * 2560; err = 0.34179688 * 2560; time = 0.0166s; samplesPerSecond = 154179.7
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.20726929 * 2560; err = 0.37070313 * 2560; time = 0.0166s; samplesPerSecond = 153790.7
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: ce = 1.15195923 * 2560; err = 0.34843750 * 2560; time = 0.0166s; samplesPerSecond = 154142.6
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.22251282 * 2560; err = 0.35742188 * 2560; time = 0.0166s; samplesPerSecond = 154300.5
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.10512390 * 2560; err = 0.34062500 * 2560; time = 0.0166s; samplesPerSecond = 154086.9
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.17382812 * 2560; err = 0.36250000 * 2560; time = 0.0166s; samplesPerSecond = 154086.9
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: ce = 1.15955811 * 2560; err = 0.35390625 * 2560; time = 0.0166s; samplesPerSecond = 153883.1
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.11036987 * 2560; err = 0.34140625 * 2560; time = 0.0166s; samplesPerSecond = 153985.0
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.17425537 * 2560; err = 0.34375000 * 2560; time = 0.0167s; samplesPerSecond = 153670.7
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.11206360 * 2560; err = 0.32929687 * 2560; time = 0.0166s; samplesPerSecond = 154319.1
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: ce = 1.13883057 * 2560; err = 0.35000000 * 2560; time = 0.0166s; samplesPerSecond = 154096.2
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.14698486 * 2560; err = 0.33984375 * 2560; time = 0.0166s; samplesPerSecond = 154059.1
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.11311951 * 2560; err = 0.32929687 * 2560; time = 0.0166s; samplesPerSecond = 154114.7
07/14/2016 14:13:27:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.15510254 * 2560; err = 0.34218750 * 2560; time = 0.0166s; samplesPerSecond = 154142.6
07/14/2016 14:13:27: Finished Epoch[ 1 of 4]: [Training] ce = 1.39406672 * 81920; err = 0.39781494 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.622809s
07/14/2016 14:13:27: SGD: Saving checkpoint model '/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.1'

07/14/2016 14:13:27: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/14/2016 14:13:27: Starting minibatch loop.
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.36421261 * 5120; err = 0.39589844 * 5120; time = 0.0284s; samplesPerSecond = 180104.1
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.61919937 * 5120; err = 0.41816406 * 5120; time = 0.0259s; samplesPerSecond = 197675.8
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.23572865 * 5120; err = 0.38203125 * 5120; time = 0.0258s; samplesPerSecond = 198280.5
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.20050049 * 5120; err = 0.37226562 * 5120; time = 0.0259s; samplesPerSecond = 197713.9
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.14316940 * 5120; err = 0.34902344 * 5120; time = 0.0258s; samplesPerSecond = 198157.8
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.13801956 * 5120; err = 0.35332031 * 5120; time = 0.0258s; samplesPerSecond = 198180.8
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.08585892 * 5120; err = 0.33671875 * 5120; time = 0.0258s; samplesPerSecond = 198342.0
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.08451385 * 5120; err = 0.32558594 * 5120; time = 0.0258s; samplesPerSecond = 198234.5
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.09470139 * 5120; err = 0.33710937 * 5120; time = 0.0258s; samplesPerSecond = 198357.4
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.10867310 * 5120; err = 0.33867188 * 5120; time = 0.0259s; samplesPerSecond = 197958.6
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.09835815 * 5120; err = 0.34824219 * 5120; time = 0.0258s; samplesPerSecond = 198157.8
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.06938324 * 5120; err = 0.33808594 * 5120; time = 0.0258s; samplesPerSecond = 198734.6
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.07041473 * 5120; err = 0.33183594 * 5120; time = 0.0259s; samplesPerSecond = 197843.8
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.06565399 * 5120; err = 0.32832031 * 5120; time = 0.0259s; samplesPerSecond = 197713.9
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.06558990 * 5120; err = 0.32832031 * 5120; time = 0.0258s; samplesPerSecond = 198226.8
07/14/2016 14:13:27:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06581879 * 5120; err = 0.32851562 * 5120; time = 0.0259s; samplesPerSecond = 198012.1
07/14/2016 14:13:27: Finished Epoch[ 2 of 4]: [Training] ce = 1.15686226 * 81920; err = 0.35075684 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.419306s
07/14/2016 14:13:27: SGD: Saving checkpoint model '/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.2'

07/14/2016 14:13:27: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

07/14/2016 14:13:27: Starting minibatch loop.
07/14/2016 14:13:27:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.07776337 * 5120; err = 0.32871094 * 5120; time = 0.0266s; samplesPerSecond = 192242.7
07/14/2016 14:13:27:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.08869438 * 5120; err = 0.33007812 * 5120; time = 0.0259s; samplesPerSecond = 197759.8
07/14/2016 14:13:27:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.08837357 * 5120; err = 0.33964844 * 5120; time = 0.0259s; samplesPerSecond = 197462.3
07/14/2016 14:13:27:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.17116623 * 5120; err = 0.35937500 * 5120; time = 0.0258s; samplesPerSecond = 198196.1
07/14/2016 14:13:27:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.06909828 * 5120; err = 0.32910156 * 5120; time = 0.0259s; samplesPerSecond = 197882.0
07/14/2016 14:13:27:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.10130157 * 5120; err = 0.34335938 * 5120; time = 0.0259s; samplesPerSecond = 197713.9
07/14/2016 14:13:28:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.07150497 * 5120; err = 0.33554688 * 5120; time = 0.0258s; samplesPerSecond = 198073.4
07/14/2016 14:13:28:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.06556396 * 5120; err = 0.33398438 * 5120; time = 0.0259s; samplesPerSecond = 198065.8
07/14/2016 14:13:28:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.05110626 * 5120; err = 0.33027344 * 5120; time = 0.0258s; samplesPerSecond = 198265.2
07/14/2016 14:13:28:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.10985947 * 5120; err = 0.34550781 * 5120; time = 0.0259s; samplesPerSecond = 197614.7
07/14/2016 14:13:28:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.04764252 * 5120; err = 0.32324219 * 5120; time = 0.0258s; samplesPerSecond = 198196.1
07/14/2016 14:13:28:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.04093933 * 5120; err = 0.32480469 * 5120; time = 0.0258s; samplesPerSecond = 198403.5
07/14/2016 14:13:28:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.08466644 * 5120; err = 0.32812500 * 5120; time = 0.0259s; samplesPerSecond = 197752.1
07/14/2016 14:13:28:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.08865051 * 5120; err = 0.32812500 * 5120; time = 0.0259s; samplesPerSecond = 197966.2
07/14/2016 14:13:28:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.04417267 * 5120; err = 0.33007812 * 5120; time = 0.0259s; samplesPerSecond = 197958.6
07/14/2016 14:13:28:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.03673096 * 5120; err = 0.31679687 * 5120; time = 0.0258s; samplesPerSecond = 198142.4
07/14/2016 14:13:28: Finished Epoch[ 3 of 4]: [Training] ce = 1.07732716 * 81920; err = 0.33292236 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.417702s
07/14/2016 14:13:28: SGD: Saving checkpoint model '/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.3'

07/14/2016 14:13:28: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

07/14/2016 14:13:28: Starting minibatch loop.
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.05172520 * 5120; err = 0.33027344 * 5120; time = 0.0266s; samplesPerSecond = 192720.3
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.10188996 * 4700; err = 0.34829787 * 4700; time = 0.0535s; samplesPerSecond = 87926.1
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.02134304 * 5120; err = 0.32421875 * 5120; time = 0.0259s; samplesPerSecond = 197698.7
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.04017105 * 5120; err = 0.32617188 * 5120; time = 0.0259s; samplesPerSecond = 197927.9
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.03495445 * 5120; err = 0.32597656 * 5120; time = 0.0258s; samplesPerSecond = 198249.8
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.03636284 * 5120; err = 0.32460937 * 5120; time = 0.0259s; samplesPerSecond = 197614.7
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.02514038 * 5120; err = 0.32421875 * 5120; time = 0.0258s; samplesPerSecond = 198111.7
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.00444717 * 5120; err = 0.30800781 * 5120; time = 0.0259s; samplesPerSecond = 197905.0
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.02353134 * 5120; err = 0.31699219 * 5120; time = 0.0258s; samplesPerSecond = 198295.9
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.98177795 * 5120; err = 0.30917969 * 5120; time = 0.0258s; samplesPerSecond = 198142.4
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 0.99231262 * 5120; err = 0.30058594 * 5120; time = 0.0258s; samplesPerSecond = 198173.1
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 0.98494034 * 5120; err = 0.31250000 * 5120; time = 0.0259s; samplesPerSecond = 197675.8
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.00793228 * 5120; err = 0.31542969 * 5120; time = 0.0259s; samplesPerSecond = 197943.2
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.98824615 * 5120; err = 0.31054688 * 5120; time = 0.0259s; samplesPerSecond = 197775.0
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.00435791 * 5120; err = 0.31191406 * 5120; time = 0.0259s; samplesPerSecond = 197973.9
07/14/2016 14:13:28:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.00445404 * 5120; err = 0.31582031 * 5120; time = 0.0259s; samplesPerSecond = 197843.8
07/14/2016 14:13:28: Finished Epoch[ 4 of 4]: [Training] ce = 1.01861782 * 81920; err = 0.31893311 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.447484s
07/14/2016 14:13:28: SGD: Saving checkpoint model '/tmp/cntk-test-20160714141322.924935/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech'
07/14/2016 14:13:28: CNTKCommandTrainEnd: speechTrain

07/14/2016 14:13:28: Action "train" complete.

07/14/2016 14:13:28: __COMPLETED__